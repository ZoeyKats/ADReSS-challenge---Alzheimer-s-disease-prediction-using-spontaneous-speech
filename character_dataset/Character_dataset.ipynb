{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "rc3zkK5sDN2v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l524YuVDENT",
        "outputId": "449d1ba8-1134-4ca5-d1b2-3647c7f1d081"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "y_test_id = []\n",
        "\n",
        "y_train_cc = []\n",
        "y_train_cd = [] #seperate for the beggining beacuse the files will be seperate\n",
        "\n",
        "x_train_cc_ids = []\n",
        "x_train_cd_ids = []"
      ],
      "metadata": {
        "id": "lrDlrEewDKSr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'ID': [],\n",
        "        'Difficulties' : [],\n",
        "        'uh': [],\n",
        "        'retrace_no_corr': [],\n",
        "        'retrace_with_corr': [],\n",
        "        'interruption': [],\n",
        "        'gfi': [],\n",
        "        # 'ari': [],\n",
        "        'unique_words': [],\n",
        "        'text_len': [],\n",
        "        # 'bag_of_words': [],\n",
        "        'prediction' : []}"
      ],
      "metadata": {
        "id": "mznUcfKHDME_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δημιουργία χαρακτηριστικών\n",
        "\n",
        "-------------------------------------\n",
        "\n",
        "Functions to create characteristics"
      ],
      "metadata": {
        "id": "tUgyQgxygdnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ari(sentences_from_file):\n",
        "  all_sentences = find_all_sentences(sentences_from_file)\n",
        "  total_characters = 0\n",
        "  total_words = 0\n",
        "  total_sentences = len(all_sentences)\n",
        "  for s in all_sentences:\n",
        "    for word in s:\n",
        "      total_words += 1\n",
        "      total_characters += len(word)\n",
        "\n",
        "  ari = 4.71 * (total_characters / (total_words * 1.0)) + 0.5 * (total_words / (total_sentences * 1.0)) - 21.43\n",
        "  return ari"
      ],
      "metadata": {
        "id": "dFlM6p8eDOOY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_sentences (sentences_from_file):\n",
        "  regex = \"[^a-zA-Z.?!']+\"\n",
        "  p=re.compile(regex)\n",
        "  all_sentences = []\n",
        "  sentence = []\n",
        "  for s in sentences_from_file:\n",
        "    s_split = s.split()\n",
        "    for word in s_split:\n",
        "      if (not re.search(p, word)):\n",
        "        if word != \".\" and word != \"!\" and word != \"?\":\n",
        "          sentence.append(word)\n",
        "        elif word == \".\" or word == \"!\" or word == \"?\":\n",
        "          if len(sentence) != 0:\n",
        "            all_sentences.append(sentence)\n",
        "          sentence = []\n",
        "\n",
        "  if len(sentence) != 0:\n",
        "    all_sentences.append(sentence)\n",
        "\n",
        "  return all_sentences\n"
      ],
      "metadata": {
        "id": "pxSCUKq9DOjO"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gfi(sentences_from_file):\n",
        "  hard_words = 0\n",
        "  all_words = []\n",
        "  all_sentences_actually = find_all_sentences(sentences_from_file)\n",
        "  for s in all_sentences_actually:\n",
        "    for word in s:\n",
        "      all_words.append(word)\n",
        "      if(len(word)>=7):\n",
        "        hard_words += 1\n",
        "\n",
        "  total_words = len(all_words)\n",
        "  total_sentences = len(all_sentences_actually)\n",
        "  avg_words_per_sentence = total_words / (total_sentences * 1.0)\n",
        "  hard_words_percent = hard_words / (total_sentences * 1.0)\n",
        "  gfi = 0.4 * (avg_words_per_sentence + 100 * hard_words_percent)\n",
        "  return gfi"
      ],
      "metadata": {
        "id": "VBkudEKUDO5F"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unique_words(sentences_from_file):\n",
        "  all_words = []\n",
        "  for sentence in sentences_from_file:\n",
        "    for word in sentence:\n",
        "      all_words.append(word)\n",
        "\n",
        "  unique_words = set(all_words)\n",
        "  return len(set(all_words)) / (len(all_words) * 1.0)"
      ],
      "metadata": {
        "id": "5Ov1xfF2DPFL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_len(sentences_from_file):\n",
        "  all_words = []\n",
        "  for sentence in sentences_from_file:\n",
        "    for word in sentence:\n",
        "      all_words.append(word)\n",
        "\n",
        "  return len(set(all_words))"
      ],
      "metadata": {
        "id": "NVHme_V7DPLj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ανάγνωση των δεδομένων εκπαίδευσης και δημιουργία dataset. Κατά την διάρκεια της διαπέρασης των δεδομένων, υπολογίζονται και άλλα χαρακτηριστικά, όπως τα χαρακτηριστικά της κωδικοποίησης CHAT\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "Reading training data and creating the dataset. While reading the data, other characteristics are calculated, like characteristics based on CHAT encoding"
      ],
      "metadata": {
        "id": "q_A5t0PQgmhf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = glob.glob(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription\"+\"/*/*.cha\")\n",
        "words_per_participant_cc = []\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "for filename in path:\n",
        "  # print(\"-------------------------------------\", filename, \"-------------------------------------\")\n",
        "  train_file = open(filename, \"r\")\n",
        "  print(filename)\n",
        "\n",
        "  keep_filename = filename\n",
        "  keep_filename = keep_filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/\", \"\")\n",
        "  keep_filename = keep_filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/c/\", \"\")\n",
        "  keep_filename = keep_filename.replace(\".cha\", \"\")\n",
        "  # print(keep_filename)\n",
        "  x_train_cc_ids.append(keep_filename)\n",
        "\n",
        "  train_text_non_AD = train_file.read()\n",
        "  # x_train.append(train_text_non_AD)\n",
        "  cc_or_cd = filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/\", \"\")\n",
        "  if \"cc\" in cc_or_cd:\n",
        "    y_train_cc.append(0)\n",
        "  else :\n",
        "    y_train_cc.append(1)\n",
        "\n",
        "\n",
        "  sentences = train_text_non_AD.split(\"\\n\")\n",
        "  regex = \"[^a-zA-Z]+\" #this is so that i can seperate the words and take out the symbols\n",
        "  p=re.compile(regex)\n",
        "\n",
        "  par_talk_sentences = []\n",
        "  begin_answer = False\n",
        "  for item in sentences:\n",
        "    # print(item)\n",
        "    if item.startswith('*PAR'):\n",
        "      # print(item)\n",
        "      par_talk_sentences.append(item)\n",
        "      begin_answer = True #check if a *PAR has started in the previous line\n",
        "\n",
        "    elif begin_answer == True:\n",
        "      if not item.startswith('*') and not item.startswith('%') and not item.startswith('@'): #if it has started but has one of those symbol, it ends (else)\n",
        "        # print(item)\n",
        "        par_talk_sentences.append(item)\n",
        "        # if(re.search(p, item)):\n",
        "        #   print(item)\n",
        "      else:\n",
        "         begin_answer = False\n",
        "\n",
        "    elif (begin_answer==False):\n",
        "      pass\n",
        "\n",
        "  talker_words = []\n",
        "  difficulty_words = 0\n",
        "  no_uhs = 0\n",
        "  retrace_no_correction = 0\n",
        "  retrace_with_correction = 0\n",
        "  interruptions = 0\n",
        "  sentence_word_count = 0\n",
        "  gfi = calculate_gfi(par_talk_sentences)\n",
        "  text_length = get_text_len(par_talk_sentences)\n",
        "  # ari = calculate_ari(par_talk_sentences)\n",
        "  unique = find_unique_words(par_talk_sentences)\n",
        "  # print(\"gfi : \",  gfi)\n",
        "  for item in par_talk_sentences:\n",
        "    all_words = item.split()\n",
        "    # print(all_words)\n",
        "    for word in all_words:\n",
        "      if(not re.search(p, word)):\n",
        "        talker_words.append(word)\n",
        "        # print(word)\n",
        "      # print (\"------------------------------------------------------------------------------------------------------------------------\")\n",
        "      # print(word, re.search(p, word))\n",
        "      else:\n",
        "        if \"'\" in word:\n",
        "          new_word = word.replace(\"'\", \"\")\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "            # print(new_word)\n",
        "        elif \"(\" in word:\n",
        "          new_word = word.replace(\"(\", \"\")\n",
        "          new_word = new_word.replace(\")\", \"\")\n",
        "          difficulty_words += 1\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"&uh\" in word:\n",
        "          no_uhs += 1\n",
        "          new_word = word.replace(\"&\", \"\")\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"[/]\" in word:\n",
        "          retrace_no_correction += 1\n",
        "        elif \"[//]\" in word:\n",
        "          retrace_with_correction += 1\n",
        "        elif \"+//\" in word:\n",
        "          interruptions += 1\n",
        "\n",
        "\n",
        "\n",
        "    lemmatized_talker_words = []\n",
        "    for word in talker_words:\n",
        "      if word != \".\" and word != \"?\" and word != \"!\":\n",
        "        lemmatized_talker_words.append(nlp(word)[0].lemma_)\n",
        "      else:\n",
        "        lemmatized_talker_words.append(word)\n",
        "      # print(word, \" : \", nlp(word)[0].lemma_)\n",
        "\n",
        "  # calculate_gfi(par_talk_sentences)\n",
        "  words_per_participant_cc.append(lemmatized_talker_words)\n",
        "\n",
        "  # vectorizer = CountVectorizer(vocabulary=set(lemmatized_talker_words))\n",
        "  # corpus = ['woman', 'window']\n",
        "  # X = vectorizer.transform(corpus)\n",
        "  # print(X.toarray())\n",
        "\n",
        "  # print(lemmatized_talker_words)\n",
        "  # print(\" == \", y_train_cc[-1])\n",
        "  # print(\"Difficculty\", difficulty_words)\n",
        "  data['ID'].append(keep_filename)\n",
        "  data['Difficulties'].append(difficulty_words/(text_length * 1.0))\n",
        "  # data['Difficulties'].append(difficulty_words)\n",
        "  data['uh'].append(no_uhs)\n",
        "\n",
        "  if \"cc\" in cc_or_cd:\n",
        "    data['prediction'].append(0)\n",
        "  else :\n",
        "    data['prediction'].append(1)\n",
        "\n",
        "  data['retrace_no_corr'].append(retrace_no_correction)\n",
        "  data['retrace_with_corr'].append(retrace_with_correction)\n",
        "  data['interruption'].append(interruptions)\n",
        "  data['gfi'].append(gfi)\n",
        "  # data['ari'].append(ari)\n",
        "  data['unique_words'].append(unique)\n",
        "  data['text_len'].append(text_length)\n",
        "  # data['bag_of_words'].append(X.toarray())\n",
        "  # data['age'].append(-1)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuzHlm-ODZxf",
        "outputId": "16c86481-196d-46aa-acfb-04bc1b364379"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S097.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S080.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S086.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S110.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S082.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S087.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S083.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S116.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S118.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S089.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S081.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S104.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S090.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S100.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S079.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S111.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S114.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S107.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S093.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S101.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S092.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S103.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S108.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S095.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S084.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S094.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S096.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S130.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S151.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S154.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S122.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S137.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S132.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S126.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S145.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S125.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S156.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S135.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S124.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S127.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S141.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S144.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S143.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S153.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S129.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S148.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S128.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S149.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S138.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S140.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S139.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S136.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S142.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/S150.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S003.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S007.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S001.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S005.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S009.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S004.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S002.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S006.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S015.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S027.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S032.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S011.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S049.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S059.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S058.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S041.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S030.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S017.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S048.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S064.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S061.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S055.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S035.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S067.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S012.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S063.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S039.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S013.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S029.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S028.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S018.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S020.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S025.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S043.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S021.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S033.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S038.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S019.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S024.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S056.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S040.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S016.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S051.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S052.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S036.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S034.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S062.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S068.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S072.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S076.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S077.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S070.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S073.cha\n",
            "/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/S071.cha\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.columns)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p561uhmVDaFn",
        "outputId": "0fea243a-4fd0-4050-cd77-0c291345aeb2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Difficulties', 'uh', 'retrace_no_corr', 'retrace_with_corr',\n",
            "       'interruption', 'gfi', 'unique_words', 'text_len', 'prediction'],\n",
            "      dtype='object')\n",
            "                                                    ID  Difficulties  uh  \\\n",
            "0    /content/drive/MyDrive/ADReSS-IS2020-train/ADR...      0.018182   1   \n",
            "1    /content/drive/MyDrive/ADReSS-IS2020-train/ADR...      0.020000   1   \n",
            "2    /content/drive/MyDrive/ADReSS-IS2020-train/ADR...      0.053571   2   \n",
            "3    /content/drive/MyDrive/ADReSS-IS2020-train/ADR...      0.072727   1   \n",
            "4    /content/drive/MyDrive/ADReSS-IS2020-train/ADR...      0.050000  24   \n",
            "..                                                 ...           ...  ..   \n",
            "103                                               S076      0.018519   2   \n",
            "104                                               S077      0.018182   1   \n",
            "105                                               S070      0.000000   3   \n",
            "106                                               S073      0.054545   5   \n",
            "107                                               S071      0.000000   1   \n",
            "\n",
            "     retrace_no_corr  retrace_with_corr  interruption        gfi  \\\n",
            "0                  1                  3             0  41.100000   \n",
            "1                  1                  0             0  12.742857   \n",
            "2                  1                  1             0  39.353846   \n",
            "3                  1                  0             0  16.566667   \n",
            "4                 19                 10             0  48.133333   \n",
            "..               ...                ...           ...        ...   \n",
            "103                0                  2             0  35.709091   \n",
            "104                0                  5             0  39.200000   \n",
            "105                1                  2             0  49.636364   \n",
            "106                1                  1             0  30.492308   \n",
            "107                0                  0             0  52.650000   \n",
            "\n",
            "     unique_words  text_len  prediction  \n",
            "0        0.051643        55           1  \n",
            "1        0.146199        50           1  \n",
            "2        0.074369        56           1  \n",
            "3        0.065632        55           1  \n",
            "4        0.028382        60           1  \n",
            "..            ...       ...         ...  \n",
            "103      0.074689        54           0  \n",
            "104      0.041793        55           0  \n",
            "105      0.076023        52           0  \n",
            "106      0.064706        55           0  \n",
            "107      0.101512        47           0  \n",
            "\n",
            "[108 rows x 10 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = {'ID': [],\n",
        "        'Difficulties' : [],\n",
        "        'uh': [],\n",
        "        'retrace_no_corr': [],\n",
        "        'retrace_with_corr': [],\n",
        "        'interruption': [],\n",
        "        'gfi': [],\n",
        "        # 'ari': [],\n",
        "        'unique_words': [],\n",
        "        'text_len': [],\n",
        "        # 'bag_of_words': [],\n",
        "        'prediction' : []}"
      ],
      "metadata": {
        "id": "kicCdrBgDaWU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Ανάγνωση των δεδομένων ελέγχου και δημιουργία dataset. Κατά την διάρκεια της διαπέρασης των δεδομένων, υπολογίζονται και άλλα χαρακτηριστικά, όπως τα χαρακτηριστικά της κωδικοποίησης CHAT.\n",
        "- Η μόνη διαφορά με τα δεδομένα εκπαίδευσης είναι ότι τα δεδομένα ελέγχου έχουν το prediction στο αρχείο labels.txt μαζί με κάποια άλλα χαρακτηριστικά, επομένως υπολογίζονται πρώτα τα χαρακτηριστικά και παρακάτω θα συνδεθούν με τα predictions\n",
        "\n",
        "-----------------------------------------------------------------------\n",
        "\n",
        "- Reading test data and creating the dataset. While reading the data, other characteristics are calculated, like characteristics based on CHAT encoding\n",
        "- The only difference with training data, is that the predictions of the train data are saved on the file labels.txt along with some other data, so at first the characteristics are calculated and after they will be connected to the predictions  "
      ],
      "metadata": {
        "id": "777LKh3Gg_CZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "path = glob.glob(\"/content/drive/MyDrive/ADReSS-IS2020-test/ADReSS-IS2020-data/test/transcription\"+\"/*.cha\")\n",
        "words_per_participant_test = []\n",
        "\n",
        "for filename in path:\n",
        "  # print(\"-------------------------------------\", filename, \"-------------------------------------\")\n",
        "  test_file = open(filename, \"r\")\n",
        "  keep_filename = filename\n",
        "  keep_filename = keep_filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-test/ADReSS-IS2020-data/test/transcription/\", \"\")\n",
        "  keep_filename = keep_filename.replace(\".cha\", \"\")\n",
        "  # print(keep_filename)\n",
        "  # print(filename)\n",
        "  test_text_all = test_file.read()\n",
        "  # print(test_text_all)\n",
        "\n",
        "  sentences = test_text_all.split(\"\\n\")\n",
        "  per_talk_sentences_test = []\n",
        "  begin_answer = False\n",
        "  for item in sentences:\n",
        "    if item.startswith('*PAR'):\n",
        "      per_talk_sentences_test.append(item)\n",
        "      begin_answer = True\n",
        "\n",
        "    elif begin_answer == True:\n",
        "      if not item.startswith('*') and not item.startswith('%') and not item.startswith('@'):\n",
        "        per_talk_sentences_test.append(item)\n",
        "\n",
        "      else:\n",
        "        begin_answer = False\n",
        "\n",
        "    elif begin_answer == False:\n",
        "      pass\n",
        "\n",
        "  talker_words = []\n",
        "  difficulty_words = 0\n",
        "  no_uhs = 0\n",
        "  retrace_no_correction = 0\n",
        "  retrace_with_correction = 0\n",
        "  interruptions = 0\n",
        "  gfi = calculate_gfi(per_talk_sentences_test)\n",
        "  # ari = calculate_ari(per_talk_sentences_test)\n",
        "  unique = find_unique_words(par_talk_sentences)\n",
        "  text_length = get_text_len(par_talk_sentences)\n",
        "  # print(\"gfi : \",  gfi)\n",
        "  for item in per_talk_sentences_test:\n",
        "    all_words_test = item.split()\n",
        "\n",
        "    for word in all_words_test:\n",
        "      if not re.search(p, word):\n",
        "        talker_words.append(word)\n",
        "\n",
        "      else:\n",
        "        if \"'\" in word:\n",
        "          new_word = word.replace(\"'\", \"\")\n",
        "          if not re.search(p, new_word):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"(\" in word:\n",
        "          new_word = word.replace(\"(\", \"\")\n",
        "          new_word = new_word.replace(\")\", \"\")\n",
        "          difficulty_words += 1\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"&uh\" in word:\n",
        "          no_uhs += 1\n",
        "          new_word = word.replace(\"&\", \"\")\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"[/]\" in word:\n",
        "          retrace_no_correction += 1\n",
        "        elif \"[//]\" in word:\n",
        "          retrace_with_correction += 1\n",
        "        elif \"+//\" in word:\n",
        "          interruptions += 1\n",
        "\n",
        "    lemmatized_talker_words_test = []\n",
        "    for word in talker_words:\n",
        "      lemmatized_talker_words_test.append(nlp(word)[0].lemma_)\n",
        "\n",
        "\n",
        "  test_data['ID'].append(keep_filename)\n",
        "  test_data['Difficulties'].append(difficulty_words/(text_length * 1.0))\n",
        "  # test_data['Difficulties'].append(difficulty_words)\n",
        "  test_data['uh'].append(no_uhs)\n",
        "  test_data['prediction'].append(-1)\n",
        "  test_data['retrace_no_corr'].append(retrace_no_correction)\n",
        "  test_data['retrace_with_corr'].append(retrace_with_correction)\n",
        "  test_data['interruption'].append(interruptions)\n",
        "  test_data['gfi'].append(gfi)\n",
        "  # test_data['ari'].append(ari)\n",
        "  test_data['unique_words'].append(unique)\n",
        "  test_data['text_len'].append(text_length)\n",
        "  # test_data['bag_of_words'].append(bag_of_words)\n",
        "  # test_data['age'].append(-1)\n",
        "\n",
        "  words_per_participant_test.append(lemmatized_talker_words_test)\n",
        "  # print(lemmatized_talker_words_test)\n"
      ],
      "metadata": {
        "id": "a1_SuA8_Dalg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_t = pd.DataFrame(test_data)\n",
        "print(df_t.columns)\n",
        "print(df_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN0L378IDa5D",
        "outputId": "886218f6-efbb-4f97-f298-05e7295a37d2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Difficulties', 'uh', 'retrace_no_corr', 'retrace_with_corr',\n",
            "       'interruption', 'gfi', 'unique_words', 'text_len', 'prediction'],\n",
            "      dtype='object')\n",
            "      ID  Difficulties  uh  retrace_no_corr  retrace_with_corr  interruption  \\\n",
            "0   S172      0.000000   3                0                  3             0   \n",
            "1   S171      0.063830   5                1                  3             0   \n",
            "2   S174      0.063830  10                1                  3             0   \n",
            "3   S166      0.042553   5                2                  4             0   \n",
            "4   S160      0.042553   3                0                  0             0   \n",
            "5   S161      0.106383   2                0                  0             1   \n",
            "6   S167      0.319149   5                4                  6             0   \n",
            "7   S170      0.085106   1                0                  1             0   \n",
            "8   S168      0.127660   3                1                  0             0   \n",
            "9   S165      0.127660   3                2                  0             0   \n",
            "10  S173      0.085106   2                1                  2             0   \n",
            "11  S164      0.063830   1                1                  1             0   \n",
            "12  S169      0.000000   0                0                  0             0   \n",
            "13  S163      0.085106   8                1                  2             0   \n",
            "14  S162      0.021277   3                0                  2             0   \n",
            "15  S182      0.063830   0                2                  1             0   \n",
            "16  S180      0.021277   2                1                  0             0   \n",
            "17  S201      0.000000   1                0                  0             0   \n",
            "18  S179      0.063830   1                0                  2             1   \n",
            "19  S178      0.106383   1                0                  2             0   \n",
            "20  S189      0.063830   2                3                  2             0   \n",
            "21  S184      0.021277   4                0                  1             0   \n",
            "22  S175      0.361702   1                2                  1             0   \n",
            "23  S187      0.425532   8                7                  9             0   \n",
            "24  S188      0.021277   1                0                  2             0   \n",
            "25  S205      0.000000   1                0                  1             0   \n",
            "26  S198      0.085106   0                0                  1             0   \n",
            "27  S206      0.085106   2                1                  0             0   \n",
            "28  S191      0.191489   4                4                  4             0   \n",
            "29  S177      0.000000   2                0                  4             0   \n",
            "30  S197      0.042553   2                0                  0             0   \n",
            "31  S203      0.148936   1                2                  0             0   \n",
            "32  S204      0.000000   3                0                  0             0   \n",
            "33  S193      0.021277   7                0                  1             0   \n",
            "34  S194      0.063830   1                2                  0             0   \n",
            "35  S192      0.276596   2                6                  3             0   \n",
            "36  S196      0.106383   0                0                  1             0   \n",
            "37  S186      0.042553   1                0                  0             0   \n",
            "38  S202      0.063830   5                3                  1             0   \n",
            "39  S183      0.000000   2                0                  3             0   \n",
            "40  S195      0.000000   0                1                  0             0   \n",
            "41  S181      0.170213   4                1                  2             0   \n",
            "42  S185      0.191489   1                0                  4             0   \n",
            "43  S199      0.106383   0                0                  1             0   \n",
            "44  S200      0.085106   0                0                  1             0   \n",
            "45  S176      0.319149  12                5                  5             0   \n",
            "46  S190      0.170213   1                3                  3             0   \n",
            "47  S207      0.021277   1                0                  0             0   \n",
            "\n",
            "           gfi  unique_words  text_len  prediction  \n",
            "0    69.000000      0.101512        47          -1  \n",
            "1    42.800000      0.101512        47          -1  \n",
            "2    64.828571      0.101512        47          -1  \n",
            "3    75.211429      0.101512        47          -1  \n",
            "4    67.760000      0.101512        47          -1  \n",
            "5    43.476923      0.101512        47          -1  \n",
            "6    67.478261      0.101512        47          -1  \n",
            "7    60.666667      0.101512        47          -1  \n",
            "8    52.240000      0.101512        47          -1  \n",
            "9    28.942857      0.101512        47          -1  \n",
            "10   40.142857      0.101512        47          -1  \n",
            "11   17.550000      0.101512        47          -1  \n",
            "12   48.628571      0.101512        47          -1  \n",
            "13   64.200000      0.101512        47          -1  \n",
            "14   50.075000      0.101512        47          -1  \n",
            "15   46.276923      0.101512        47          -1  \n",
            "16   31.200000      0.101512        47          -1  \n",
            "17   37.485714      0.101512        47          -1  \n",
            "18   27.500000      0.101512        47          -1  \n",
            "19   49.633333      0.101512        47          -1  \n",
            "20   33.133333      0.101512        47          -1  \n",
            "21   52.914286      0.101512        47          -1  \n",
            "22   23.460870      0.101512        47          -1  \n",
            "23   21.228571      0.101512        47          -1  \n",
            "24   34.000000      0.101512        47          -1  \n",
            "25   48.857143      0.101512        47          -1  \n",
            "26   22.560000      0.101512        47          -1  \n",
            "27   62.036364      0.101512        47          -1  \n",
            "28   25.511111      0.101512        47          -1  \n",
            "29   53.094737      0.101512        47          -1  \n",
            "30   53.885714      0.101512        47          -1  \n",
            "31   19.771429      0.101512        47          -1  \n",
            "32  101.920000      0.101512        47          -1  \n",
            "33   43.350000      0.101512        47          -1  \n",
            "34   17.066667      0.101512        47          -1  \n",
            "35   24.707692      0.101512        47          -1  \n",
            "36   50.857143      0.101512        47          -1  \n",
            "37   66.800000      0.101512        47          -1  \n",
            "38   55.720000      0.101512        47          -1  \n",
            "39   40.736842      0.101512        47          -1  \n",
            "40   39.200000      0.101512        47          -1  \n",
            "41   18.693333      0.101512        47          -1  \n",
            "42   17.085714      0.101512        47          -1  \n",
            "43   51.880000      0.101512        47          -1  \n",
            "44   70.466667      0.101512        47          -1  \n",
            "45   18.746667      0.101512        47          -1  \n",
            "46   22.550000      0.101512        47          -1  \n",
            "47   37.244444      0.101512        47          -1  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Διαβασμα predictions και συνδεση τους με τα προηγούμενα δεδομένα\n",
        "\n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "Reading the predictions and connecting them to the previous data"
      ],
      "metadata": {
        "id": "Aht8BEJthiYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "filename = \"/content/drive/MyDrive/ADReSS-IS2020-test/ADReSS-IS2020-data/test/labels.txt\"\n",
        "test_file = open(filename, \"r\")\n",
        "# print(filename)\n",
        "test_text_all = test_file.read()\n",
        "\n",
        "index_id = -1\n",
        "index_label = -1\n",
        "age = -1\n",
        "\n",
        "sentences = test_text_all.split(\"\\n\")\n",
        "first_line_words = sentences[0].split(';')\n",
        "i=0\n",
        "for label in first_line_words:\n",
        "  # print(label)\n",
        "  label = label.replace(\" \", \"\")\n",
        "  # print(label)\n",
        "  if label == \"ID\":\n",
        "    index_id = i\n",
        "    # print(\"id in label: \", i)\n",
        "  elif label == \"Label\":\n",
        "    index_label = i\n",
        "    # print(\"label in label: \", i)\n",
        "  elif label == \"age\":\n",
        "    age = i\n",
        "\n",
        "  i += 1\n",
        "\n",
        "new_sentences = []\n",
        "for sentence in sentences:\n",
        "  new_sentence = []\n",
        "  sentence = sentence.split(\";\")\n",
        "  print(\" s: \", sentence)\n",
        "  for word in sentence:\n",
        "    word = word.replace(\" \", \"\")\n",
        "    new_sentence.append(word)\n",
        "\n",
        "  new_sentences.append(new_sentence)\n",
        "\n",
        "i=0\n",
        "y_test_id = []\n",
        "ages = []\n",
        "for sentence in new_sentences:\n",
        "  # print (\"sentence: \", sentence)\n",
        "\n",
        "  if i != 0:\n",
        "    y_test_id.append(sentence[index_id])\n",
        "    y_test.append(sentence[index_label])\n",
        "    ages.append(sentence[age])\n",
        "  i+= 1\n",
        "\n",
        "print(\"_________________________________________________\")\n",
        "print(y_test_id)\n",
        "print(len(new_sentences))\n",
        "print(\"len age: \", len(ages))\n",
        "print(len(y_test_id))\n",
        "\n",
        "\n",
        "i=0\n",
        "for i in range(len(y_test_id)):\n",
        "  # print(\"len: \", len(y_test_id), \"==============================================\")\n",
        "  # print(\"id: \", y_test_id[i], \" , label: \", y_test[i])\n",
        "  list_obj = [y_test_id[i]]\n",
        "  df_t.loc[df_t['ID'].isin(list_obj), 'prediction'] = y_test[i]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsFS2Iv4D4Hh",
        "outputId": "dc70ea3e-40b7-44b0-8635-07637396be72"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " s:  ['ID   ', ' age', ' gender', 'Label ', 'mmse']\n",
            " s:  ['S160 ', ' 63', ' 1 ', ' 0 ', ' 28']\n",
            " s:  ['S161 ', ' 55', ' 1 ', ' 0 ', ' 29']\n",
            " s:  ['S162 ', ' 67', ' 1 ', ' 1 ', ' 24']\n",
            " s:  ['S163 ', ' 71', ' 0 ', ' 0 ', ' 30']\n",
            " s:  ['S164 ', ' 73', ' 1 ', ' 1 ', ' 21']\n",
            " s:  ['S165 ', ' 64', ' 0 ', ' 1 ', ' 15']\n",
            " s:  ['S166 ', ' 73', ' 1 ', ' 0 ', ' 29']\n",
            " s:  ['S167 ', ' 65', ' 0 ', ' 1 ', ' 28']\n",
            " s:  ['S168 ', ' 71', ' 0 ', ' 1 ', ' 27']\n",
            " s:  ['S169 ', ' 78', ' 0 ', ' 1 ', ' 26']\n",
            " s:  ['S170 ', ' 51', ' 0 ', ' 0 ', ' 28']\n",
            " s:  ['S171 ', ' 69', ' 1 ', ' 1 ', ' 23']\n",
            " s:  ['S172 ', ' 78', ' 1 ', ' 0 ', ' 30']\n",
            " s:  ['S173 ', ' 79', ' 1 ', ' 1 ', ' 17']\n",
            " s:  ['S174 ', ' 67', ' 1 ', ' 0 ', ' 29']\n",
            " s:  ['S175 ', ' 69', ' 1 ', ' 0 ', ' 30']\n",
            " s:  ['S176 ', ' 69', ' 0 ', ' 1 ', ' 27']\n",
            " s:  ['S177 ', ' 65', ' 0 ', ' 0 ', ' 30']\n",
            " s:  ['S178 ', ' 68', ' 1 ', ' 0 ', ' 30']\n",
            " s:  ['S179 ', ' 74', ' 0 ', ' 1 ', ' 10']\n",
            " s:  ['S180 ', ' 74', ' 0 ', ' 0 ', ' 29']\n",
            " s:  ['S181 ', ' 65', ' 1 ', ' 1 ', ' 17']\n",
            " s:  ['S182 ', ' 60', ' 1 ', ' 1 ', ' 12']\n",
            " s:  ['S183 ', ' 75', ' 0 ', ' 0 ', ' 30']\n",
            " s:  ['S184 ', ' 69', ' 0 ', ' 0 ', ' 29']\n",
            " s:  ['S185 ', ' 57', ' 1 ', ' 1 ', ' 19']\n",
            " s:  ['S186 ', ' 63', ' 0 ', ' 0 ', ' 29']\n",
            " s:  ['S187 ', ' 58', ' 0 ', ' 1 ', ' 18']\n",
            " s:  ['S188 ', ' 55', ' 1 ', ' 1 ', ' 20']\n",
            " s:  ['S189 ', ' 63', ' 1 ', ' 1 ', ' 20']\n",
            " s:  ['S190 ', ' 72', ' 0 ', ' 1 ', ' 13']\n",
            " s:  ['S191 ', ' 73', ' 1 ', ' 1 ', ' 22']\n",
            " s:  ['S192 ', ' 62', ' 1 ', ' 1 ', ' 12']\n",
            " s:  ['S193 ', ' 73', ' 1 ', ' 0 ', ' 24']\n",
            " s:  ['S194 ', ' 70', ' 1 ', ' 1 ', ' 11']\n",
            " s:  ['S195 ', ' 74', ' 1 ', ' 0 ', ' 26']\n",
            " s:  ['S196 ', ' 59', ' 0 ', ' 0 ', ' 30']\n",
            " s:  ['S197 ', ' 61', ' 1 ', ' 0 ', ' 28']\n",
            " s:  ['S198 ', ' 68', ' 1 ', ' 1 ', ' 19']\n",
            " s:  ['S199 ', ' 67', ' 1 ', ' 0 ', ' 30']\n",
            " s:  ['S200 ', ' 69', ' 0 ', ' 1 ', ' 25']\n",
            " s:  ['S201 ', ' 63', ' 1 ', ' 0 ', ' 30']\n",
            " s:  ['S202 ', ' 71', ' 0 ', ' 0 ', ' 30']\n",
            " s:  ['S203 ', ' 56', ' 0 ', ' 1 ', ' 18']\n",
            " s:  ['S204 ', ' 56', ' 0 ', ' 0 ', ' 28']\n",
            " s:  ['S205 ', ' 50', ' 0 ', ' 1 ', ' 23']\n",
            " s:  ['S206 ', ' 65', ' 0 ', ' 0 ', ' 28']\n",
            " s:  ['S207 ', ' 57', ' 1 ', ' 0 ', ' 27']\n",
            "_________________________________________________\n",
            "['S160', 'S161', 'S162', 'S163', 'S164', 'S165', 'S166', 'S167', 'S168', 'S169', 'S170', 'S171', 'S172', 'S173', 'S174', 'S175', 'S176', 'S177', 'S178', 'S179', 'S180', 'S181', 'S182', 'S183', 'S184', 'S185', 'S186', 'S187', 'S188', 'S189', 'S190', 'S191', 'S192', 'S193', 'S194', 'S195', 'S196', 'S197', 'S198', 'S199', 'S200', 'S201', 'S202', 'S203', 'S204', 'S205', 'S206', 'S207']\n",
            "49\n",
            "len age:  48\n",
            "48\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αποθήκευση των dataset σε αρχεία csv\n",
        "  \n",
        "---------------------------------------------------------------------------\n",
        "\n",
        "Saving the dataset in csv files"
      ],
      "metadata": {
        "id": "ExOQilathprb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"train_data.csv\", index = False)\n",
        "df_t.to_csv(\"test_data.csv\", index = False)"
      ],
      "metadata": {
        "id": "N_ftdhxjftuC"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}
