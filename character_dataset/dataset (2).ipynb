{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l524YuVDENT",
        "outputId": "6e35cf80-89b2-48f0-eac3-d60178785bb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Το καθαρό αρχείο χωρίς όλα τα σχόλια και το tf-idf που δεν εχω αξιοποιήσει.\n",
        "Αυτά μπορούν να βρεθούν στο αρχείο PTYXIAKH.ipynb"
      ],
      "metadata": {
        "id": "N05I4-mvEaF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "x_test = []\n",
        "y_test = []\n",
        "y_test_id = []\n",
        "\n",
        "y_train_cc = []\n",
        "y_train_cd = [] #seperate for the beggining beacuse the files will be seperate\n",
        "\n",
        "x_train_cc_ids = []\n",
        "x_train_cd_ids = []"
      ],
      "metadata": {
        "id": "lrDlrEewDKSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'ID': [],\n",
        "        'Difficulties' : [],#να τα βαλω με frequency αντι για νουμερα\n",
        "        'uh': [],\n",
        "        'retrace_no_corr': [],\n",
        "        'retrace_with_corr': [],\n",
        "        'interruption': [],\n",
        "        'gfi': [],\n",
        "        'ari': [],\n",
        "        'unique_words': [],\n",
        "        'text_len': [],\n",
        "        'stop_words': [],\n",
        "        # 'bag_of_words': [],\n",
        "        'prediction' : []}"
      ],
      "metadata": {
        "id": "mznUcfKHDME_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = [\n",
        "    \"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\",\n",
        "    \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\",\n",
        "    \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\",\n",
        "    \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\",\n",
        "    \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\",\n",
        "    \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\",\n",
        "    \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\",\n",
        "    \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\",\n",
        "    \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\",\n",
        "    \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\",\n",
        "    \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\",\n",
        "    \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"\n",
        "]"
      ],
      "metadata": {
        "id": "EXmWwf3m0aFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import re\n",
        "import spacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "rc3zkK5sDN2v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Δημιουργία χαρακτηριστικών\n",
        "\n",
        "-------------------------------------------------\n",
        "\n",
        "Functions to create characteristics"
      ],
      "metadata": {
        "id": "xgBabmoTIbKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ari(sentences_from_file):\n",
        "  all_sentences = find_all_sentences(sentences_from_file)\n",
        "  total_characters = 0\n",
        "  total_words = 0\n",
        "  total_sentences = len(all_sentences)\n",
        "  for s in all_sentences:\n",
        "    for word in s:\n",
        "      total_words += 1\n",
        "      total_characters += len(word)\n",
        "\n",
        "  ari = 4.71 * (total_characters / (total_words * 1.0)) + 0.5 * (total_words / (total_sentences * 1.0)) - 21.43\n",
        "  return ari"
      ],
      "metadata": {
        "id": "dFlM6p8eDOOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_all_sentences (sentences_from_file):\n",
        "  regex = \"[^a-zA-Z.?!']+\"\n",
        "  p=re.compile(regex)\n",
        "  all_sentences = []\n",
        "  sentence = []\n",
        "  for s in sentences_from_file:\n",
        "    s_split = s.split()\n",
        "    for word in s_split:\n",
        "      if (not re.search(p, word)):\n",
        "        if word != \".\" and word != \"!\" and word != \"?\":\n",
        "          sentence.append(word)\n",
        "        elif word == \".\" or word == \"!\" or word == \"?\":\n",
        "          if len(sentence) != 0:\n",
        "            all_sentences.append(sentence)\n",
        "          sentence = []\n",
        "\n",
        "  if len(sentence) != 0:\n",
        "    all_sentences.append(sentence)\n",
        "\n",
        "  return all_sentences\n"
      ],
      "metadata": {
        "id": "pxSCUKq9DOjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gfi(sentences_from_file):\n",
        "  hard_words = 0\n",
        "  all_words = []\n",
        "  all_sentences_actually = find_all_sentences(sentences_from_file)\n",
        "  for s in all_sentences_actually:\n",
        "    for word in s:\n",
        "      all_words.append(word)\n",
        "      if(len(word)>=7):\n",
        "        hard_words += 1\n",
        "\n",
        "  total_words = len(all_words)\n",
        "  total_sentences = len(all_sentences_actually)\n",
        "  avg_words_per_sentence = total_words / (total_sentences * 1.0)\n",
        "  hard_words_percent = hard_words / (total_sentences * 1.0)\n",
        "  gfi = 0.4 * (avg_words_per_sentence + 100 * hard_words_percent)\n",
        "  return gfi"
      ],
      "metadata": {
        "id": "VBkudEKUDO5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stop_words_count(talker_words):\n",
        "  counter = 0\n",
        "  for word in talker_words:\n",
        "    if word in stop_words:\n",
        "      counter += 1\n",
        "\n",
        "\n",
        "  if len(talker_words) != 0:\n",
        "    return counter\n",
        "  else:\n",
        "    return -1"
      ],
      "metadata": {
        "id": "SUZYxiFL_iqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_unique_words(sentences_from_file):\n",
        "  all_words = []\n",
        "  for sentence in sentences_from_file:\n",
        "    for word in sentence:\n",
        "      all_words.append(word)\n",
        "\n",
        "  unique_words = set(all_words)\n",
        "  return len(set(all_words)) / (len(all_words) * 1.0)"
      ],
      "metadata": {
        "id": "5Ov1xfF2DPFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_len(sentences_from_file):\n",
        "  all_words = []\n",
        "  for sentence in sentences_from_file:\n",
        "    for word in sentence:\n",
        "      all_words.append(word)\n",
        "\n",
        "  return len(set(all_words))"
      ],
      "metadata": {
        "id": "NVHme_V7DPLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ανάγνωση των δεδομένων εκπαίδευσης και δημιουργία dataset. Κατά την διάρκεια της διαπέρασης των δεδομένων, υπολογίζονται και άλλα χαρακτηριστικά, όπως τα χαρακτηριστικά της κωδικοποίησης CHAT\n",
        "\n",
        "----------------------------------------------------\n",
        "\n",
        "Reading training data and creating the dataset. While reading the data, other characteristics are calculated, like characteristics based on CHAT encoding"
      ],
      "metadata": {
        "id": "J9KPOdV2IhD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import glob\n",
        "# import re\n",
        "# import spacy\n",
        "#train data for non-AD\n",
        "path = glob.glob(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription\"+\"/*/*.cha\")\n",
        "words_per_participant_cc = []\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "for filename in path:\n",
        "  # print(\"-------------------------------------\", filename, \"-------------------------------------\")\n",
        "  train_file = open(filename, \"r\")\n",
        "  # print(filename)\n",
        "\n",
        "  keep_filename = filename\n",
        "  keep_filename = keep_filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cc/\", \"\")\n",
        "  keep_filename = keep_filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/cd/\", \"\")\n",
        "  keep_filename = keep_filename.replace(\".cha\", \"\")\n",
        "  print(keep_filename)\n",
        "  x_train_cc_ids.append(keep_filename)\n",
        "\n",
        "  train_text_non_AD = train_file.read()\n",
        "  # x_train.append(train_text_non_AD)\n",
        "  cc_or_cd = filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-train/ADReSS-IS2020-data/train/transcription/\", \"\")\n",
        "  if \"cc\" in cc_or_cd:\n",
        "    y_train_cc.append(0)\n",
        "  else :\n",
        "    y_train_cc.append(1)\n",
        "\n",
        "\n",
        "  sentences = train_text_non_AD.split(\"\\n\")\n",
        "  regex = \"[^a-zA-Z]+\" #this is so that i can seperate the words and take out the symbols\n",
        "  p=re.compile(regex)\n",
        "\n",
        "  par_talk_sentences = []\n",
        "  begin_answer = False\n",
        "  for item in sentences:\n",
        "    # print(item)\n",
        "    if item.startswith('*PAR'):\n",
        "      # print(item)\n",
        "      par_talk_sentences.append(item)\n",
        "      begin_answer = True #check if a *PAR has started in the previous line\n",
        "\n",
        "    elif begin_answer == True:\n",
        "      if not item.startswith('*') and not item.startswith('%') and not item.startswith('@'): #if it has started but has one of those symbol, it ends (else)\n",
        "        # print(item)\n",
        "        par_talk_sentences.append(item)\n",
        "        # if(re.search(p, item)):\n",
        "        #   print(item)\n",
        "      else:\n",
        "         begin_answer = False\n",
        "\n",
        "    elif (begin_answer==False):\n",
        "      pass\n",
        "\n",
        "  talker_words = []\n",
        "  difficulty_words = 0\n",
        "  no_uhs = 0\n",
        "  retrace_no_correction = 0\n",
        "  retrace_with_correction = 0\n",
        "  interruptions = 0\n",
        "  sentence_word_count = 0\n",
        "  gfi = calculate_gfi(par_talk_sentences)\n",
        "  text_length = get_text_len(par_talk_sentences)\n",
        "  ari = calculate_ari(par_talk_sentences)\n",
        "  unique = find_unique_words(par_talk_sentences)\n",
        "  # print(\"gfi : \",  gfi)\n",
        "  for item in par_talk_sentences:\n",
        "    all_words = item.split()\n",
        "    #HERE CALL FOR ALL WORDS\n",
        "    # print(all_words)\n",
        "    for word in all_words:\n",
        "      if(not re.search(p, word)):\n",
        "        talker_words.append(word)\n",
        "        # print(word)\n",
        "      # print (\"------------------------------------------------------------------------------------------------------------------------\")\n",
        "      # print(word, re.search(p, word))\n",
        "      else:\n",
        "        if \"'\" in word:\n",
        "          new_word = word.replace(\"'\", \"\")\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "            # print(new_word)\n",
        "        elif \"(\" in word:\n",
        "          new_word = word.replace(\"(\", \"\")\n",
        "          new_word = new_word.replace(\")\", \"\")\n",
        "          difficulty_words += 1\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"&uh\" in word:\n",
        "          no_uhs += 1\n",
        "          new_word = word.replace(\"&\", \"\")\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"[/]\" in word:\n",
        "          retrace_no_correction += 1\n",
        "        elif \"[//]\" in word:\n",
        "          retrace_with_correction += 1\n",
        "        elif \"+//\" in word:\n",
        "          interruptions += 1\n",
        "\n",
        "\n",
        "\n",
        "    lemmatized_talker_words = []\n",
        "    for word in talker_words:\n",
        "      if word != \".\" and word != \"?\" and word != \"!\":\n",
        "        lemmatized_talker_words.append(nlp(word)[0].lemma_)\n",
        "      else:\n",
        "        lemmatized_talker_words.append(word)#as paroume ta riska masssss\n",
        "      # print(word, \" : \", nlp(word)[0].lemma_)\n",
        "\n",
        "  # calculate_gfi(par_talk_sentences)\n",
        "  words_per_participant_cc.append(lemmatized_talker_words)\n",
        "  stop_ = stop_words_count(lemmatized_talker_words)\n",
        "\n",
        "  data['ID'].append(keep_filename)\n",
        "  data['Difficulties'].append(difficulty_words/(text_length * 1.0))\n",
        "  # data['Difficulties'].append(difficulty_words)\n",
        "  data['uh'].append(no_uhs)\n",
        "\n",
        "  if \"cc\" in cc_or_cd:\n",
        "    data['prediction'].append(0)\n",
        "  else :\n",
        "    data['prediction'].append(1)\n",
        "\n",
        "  data['retrace_no_corr'].append(retrace_no_correction)\n",
        "  data['retrace_with_corr'].append(retrace_with_correction)\n",
        "  data['interruption'].append(interruptions)\n",
        "  data['gfi'].append(gfi)\n",
        "  data['ari'].append(ari)\n",
        "  data['unique_words'].append(unique)\n",
        "  data['text_len'].append(text_length)\n",
        "  data['stop_words'].append(stop_)\n",
        "  # data['bag_of_words'].append(X.toarray())\n",
        "  # data['age'].append(-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuzHlm-ODZxf",
        "outputId": "c62d39c8-49cd-49e9-e1d7-99c6965897b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S097\n",
            "S080\n",
            "S086\n",
            "S110\n",
            "S082\n",
            "S087\n",
            "S083\n",
            "S116\n",
            "S118\n",
            "S089\n",
            "S081\n",
            "S104\n",
            "S090\n",
            "S100\n",
            "S079\n",
            "S111\n",
            "S114\n",
            "S107\n",
            "S093\n",
            "S101\n",
            "S092\n",
            "S103\n",
            "S108\n",
            "S095\n",
            "S084\n",
            "S094\n",
            "S096\n",
            "S130\n",
            "S151\n",
            "S154\n",
            "S122\n",
            "S137\n",
            "S132\n",
            "S126\n",
            "S145\n",
            "S125\n",
            "S156\n",
            "S135\n",
            "S124\n",
            "S127\n",
            "S141\n",
            "S144\n",
            "S143\n",
            "S153\n",
            "S129\n",
            "S148\n",
            "S128\n",
            "S149\n",
            "S138\n",
            "S140\n",
            "S139\n",
            "S136\n",
            "S142\n",
            "S150\n",
            "S003\n",
            "S007\n",
            "S001\n",
            "S005\n",
            "S009\n",
            "S004\n",
            "S002\n",
            "S006\n",
            "S015\n",
            "S027\n",
            "S032\n",
            "S011\n",
            "S049\n",
            "S059\n",
            "S058\n",
            "S041\n",
            "S030\n",
            "S017\n",
            "S048\n",
            "S064\n",
            "S061\n",
            "S055\n",
            "S035\n",
            "S067\n",
            "S012\n",
            "S063\n",
            "S039\n",
            "S013\n",
            "S029\n",
            "S028\n",
            "S018\n",
            "S020\n",
            "S025\n",
            "S043\n",
            "S021\n",
            "S033\n",
            "S038\n",
            "S019\n",
            "S024\n",
            "S056\n",
            "S040\n",
            "S016\n",
            "S051\n",
            "S052\n",
            "S036\n",
            "S034\n",
            "S062\n",
            "S068\n",
            "S072\n",
            "S076\n",
            "S077\n",
            "S070\n",
            "S073\n",
            "S071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_keys = data.keys()\n",
        "for key in all_keys:\n",
        "  if key != 'ID' and key != 'prediction' and key != 'bag_of_words':\n",
        "    the_list = data.get(key)\n",
        "    max = the_list[0]\n",
        "    min = the_list[0]\n",
        "    for item in the_list:\n",
        "      if item > max:\n",
        "        max = item\n",
        "      if item < min:\n",
        "        min = item\n",
        "\n",
        "    if max > 1:\n",
        "      new_list = []\n",
        "      for item in the_list:\n",
        "        new_value = (item - min) / ((max - min) * 1.0)\n",
        "        new_list.append(new_value)\n",
        "\n",
        "      data[key] = new_list"
      ],
      "metadata": {
        "id": "rPP_cSRg-PdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(df.columns)\n",
        "# print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p561uhmVDaFn",
        "outputId": "d287a0a1-61a7-4caf-f015-521af5d45767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Difficulties', 'uh', 'retrace_no_corr', 'retrace_with_corr',\n",
            "       'interruption', 'gfi', 'ari', 'unique_words', 'text_len', 'stop_words',\n",
            "       'prediction'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_data = {'ID': [],\n",
        "        'Difficulties' : [],\n",
        "        'uh': [],\n",
        "        'retrace_no_corr': [],\n",
        "        'retrace_with_corr': [],\n",
        "        'interruption': [],\n",
        "        'gfi': [],\n",
        "        'ari': [],\n",
        "        'unique_words': [],\n",
        "        'text_len': [],\n",
        "        'stop_words': [],\n",
        "        # 'bag_of_words': [],\n",
        "        'prediction' : []}"
      ],
      "metadata": {
        "id": "kicCdrBgDaWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ανάγνωση των δεδομένων ελέγχου και δημιουργία dataset. Κατά την διάρκεια της διαπέρασης των δεδομένων, υπολογίζονται και άλλα χαρακτηριστικά, όπως τα χαρακτηριστικά της κωδικοποίησης CHAT.\n",
        "Η μόνη διαφορά με τα δεδομένα εκπαίδευσης είναι ότι τα δεδομένα ελέγχου έχουν το prediction στο αρχείο labels.txt μαζί με κάποια άλλα χαρακτηριστικά, επομένως υπολογίζονται πρώτα τα χαρακτηριστικά και παρακάτω θα συνδεθούν με τα predictions\n",
        "\n",
        "-------------------------------------------------\n",
        "\n",
        "Reading test data and creating the dataset. While reading the data, other characteristics are calculated, like characteristics based on CHAT encoding\n",
        "The only difference with training data, is that the predictions of the train data are saved on the file labels.txt along with some other data, so at first the characteristics are calculated and after they will be connected to the predictions"
      ],
      "metadata": {
        "id": "jpV66bWIInMr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test data\n",
        "path = glob.glob(\"/content/drive/MyDrive/ADReSS-IS2020-test/ADReSS-IS2020-data/test/transcription\"+\"/*.cha\")\n",
        "words_per_participant_test = []\n",
        "\n",
        "for filename in path:\n",
        "  # print(\"-------------------------------------\", filename, \"-------------------------------------\")\n",
        "  test_file = open(filename, \"r\")\n",
        "  keep_filename = filename\n",
        "  keep_filename = keep_filename.replace(\"/content/drive/MyDrive/ADReSS-IS2020-test/ADReSS-IS2020-data/test/transcription/\", \"\")\n",
        "  keep_filename = keep_filename.replace(\".cha\", \"\")\n",
        "  print(keep_filename)\n",
        "  # print(filename)\n",
        "  test_text_all = test_file.read()\n",
        "  # print(test_text_all)\n",
        "\n",
        "  sentences = test_text_all.split(\"\\n\")\n",
        "  per_talk_sentences_test = []\n",
        "  begin_answer = False\n",
        "  for item in sentences:\n",
        "    if item.startswith('*PAR'):\n",
        "      per_talk_sentences_test.append(item)\n",
        "      begin_answer = True\n",
        "\n",
        "    elif begin_answer == True:\n",
        "      if not item.startswith('*') and not item.startswith('%') and not item.startswith('@'):\n",
        "        per_talk_sentences_test.append(item)\n",
        "\n",
        "      else:\n",
        "        begin_answer = False\n",
        "\n",
        "    elif begin_answer == False:\n",
        "      pass\n",
        "\n",
        "  talker_words = []\n",
        "  difficulty_words = 0\n",
        "  no_uhs = 0\n",
        "  retrace_no_correction = 0\n",
        "  retrace_with_correction = 0\n",
        "  interruptions = 0\n",
        "  gfi = calculate_gfi(per_talk_sentences_test)\n",
        "  ari = calculate_ari(per_talk_sentences_test)\n",
        "  unique = find_unique_words(per_talk_sentences_test)\n",
        "  text_length = get_text_len(per_talk_sentences_test)\n",
        "  # print(\"gfi : \",  gfi)\n",
        "  for item in per_talk_sentences_test:\n",
        "    all_words_test = item.split()\n",
        "\n",
        "    for word in all_words_test:\n",
        "      if not re.search(p, word):\n",
        "        talker_words.append(word)\n",
        "\n",
        "      else:\n",
        "        if \"'\" in word:\n",
        "          new_word = word.replace(\"'\", \"\")\n",
        "          if not re.search(p, new_word):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"(\" in word:\n",
        "          new_word = word.replace(\"(\", \"\")\n",
        "          new_word = new_word.replace(\")\", \"\")\n",
        "          difficulty_words += 1\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"&uh\" in word:\n",
        "          no_uhs += 1\n",
        "          new_word = word.replace(\"&\", \"\")\n",
        "          if(not re.search(p, new_word)):\n",
        "            talker_words.append(new_word)\n",
        "        elif \"[/]\" in word:\n",
        "          retrace_no_correction += 1\n",
        "        elif \"[//]\" in word:\n",
        "          retrace_with_correction += 1\n",
        "        elif \"+//\" in word:\n",
        "          interruptions += 1\n",
        "\n",
        "    lemmatized_talker_words_test = []\n",
        "    for word in talker_words:\n",
        "      lemmatized_talker_words_test.append(nlp(word)[0].lemma_)\n",
        "\n",
        "  stop_ = stop_words_count(lemmatized_talker_words_test)\n",
        "\n",
        "\n",
        "  test_data['ID'].append(keep_filename)\n",
        "  test_data['Difficulties'].append(difficulty_words/(text_length * 1.0))\n",
        "  # test_data['Difficulties'].append(difficulty_words)\n",
        "  test_data['uh'].append(no_uhs)\n",
        "  test_data['prediction'].append(-1)\n",
        "  test_data['retrace_no_corr'].append(retrace_no_correction)\n",
        "  test_data['retrace_with_corr'].append(retrace_with_correction)\n",
        "  test_data['interruption'].append(interruptions)\n",
        "  test_data['gfi'].append(gfi)\n",
        "  test_data['ari'].append(ari)\n",
        "  test_data['unique_words'].append(unique)\n",
        "  test_data['text_len'].append(text_length)\n",
        "  test_data['stop_words'].append(stop_)\n",
        "  # test_data['bag_of_words'].append(bag_of_words)\n",
        "  # test_data['age'].append(-1)\n",
        "\n",
        "  words_per_participant_test.append(lemmatized_talker_words_test)\n",
        "  # print(lemmatized_talker_words_test)\n"
      ],
      "metadata": {
        "id": "a1_SuA8_Dalg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c9ac28c-0862-4012-943a-6d6f83a0f085"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S172\n",
            "S171\n",
            "S174\n",
            "S166\n",
            "S160\n",
            "S161\n",
            "S167\n",
            "S170\n",
            "S168\n",
            "S165\n",
            "S173\n",
            "S164\n",
            "S169\n",
            "S163\n",
            "S162\n",
            "S182\n",
            "S180\n",
            "S201\n",
            "S179\n",
            "S178\n",
            "S189\n",
            "S184\n",
            "S175\n",
            "S187\n",
            "S188\n",
            "S205\n",
            "S198\n",
            "S206\n",
            "S191\n",
            "S177\n",
            "S197\n",
            "S203\n",
            "S204\n",
            "S193\n",
            "S194\n",
            "S192\n",
            "S196\n",
            "S186\n",
            "S202\n",
            "S183\n",
            "S195\n",
            "S181\n",
            "S185\n",
            "S199\n",
            "S200\n",
            "S176\n",
            "S190\n",
            "S207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_keys = test_data.keys()\n",
        "for key in all_keys:\n",
        "  if key != 'ID' and key != 'prediction' and key != 'bag_of_words':\n",
        "    the_list = test_data.get(key)\n",
        "    max = the_list[0]\n",
        "    min = the_list[0]\n",
        "    for item in the_list:\n",
        "      if item > max:\n",
        "        max = item\n",
        "      if item < min:\n",
        "        min = item\n",
        "\n",
        "    if max > 1:\n",
        "      new_list = []\n",
        "      for item in the_list:\n",
        "        if max != min:\n",
        "          new_value = (item - min) / ((max - min) * 1.0)\n",
        "        else:\n",
        "          new_value = item\n",
        "        new_list.append(new_value)\n",
        "\n",
        "      test_data[key] = new_list"
      ],
      "metadata": {
        "id": "xoC8tfXE-04O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_t = pd.DataFrame(test_data)\n",
        "print(df_t.columns)\n",
        "# print(df_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN0L378IDa5D",
        "outputId": "ed07919b-9d43-4d74-be40-a59aa7169b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['ID', 'Difficulties', 'uh', 'retrace_no_corr', 'retrace_with_corr',\n",
            "       'interruption', 'gfi', 'ari', 'unique_words', 'text_len', 'stop_words',\n",
            "       'prediction'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Διαβασμα predictions και συνδεση τους με τα προηγούμενα δεδομένα\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Reading the predictions and connecting them to the previous data"
      ],
      "metadata": {
        "id": "zYJWJ45eIuYg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#na paro to y_test\n",
        "filename = \"/content/drive/MyDrive/ADReSS-IS2020-test/ADReSS-IS2020-data/test/labels.txt\"\n",
        "test_file = open(filename, \"r\")\n",
        "# print(filename)\n",
        "test_text_all = test_file.read()\n",
        "\n",
        "index_id = -1\n",
        "index_label = -1\n",
        "age = -1\n",
        "\n",
        "sentences = test_text_all.split(\"\\n\")\n",
        "first_line_words = sentences[0].split(';')\n",
        "i=0\n",
        "for label in first_line_words:\n",
        "  # print(label)\n",
        "  label = label.replace(\" \", \"\")\n",
        "  # print(label)\n",
        "  if label == \"ID\":\n",
        "    index_id = i\n",
        "    # print(\"id in label: \", i)\n",
        "  elif label == \"Label\":\n",
        "    index_label = i\n",
        "    # print(\"label in label: \", i)\n",
        "  elif label == \"age\":\n",
        "    age = i\n",
        "\n",
        "  i += 1\n",
        "\n",
        "new_sentences = []\n",
        "for sentence in sentences:\n",
        "  new_sentence = []\n",
        "  sentence = sentence.split(\";\")\n",
        "  # print(\" s: \", sentence)\n",
        "  for word in sentence:\n",
        "    word = word.replace(\" \", \"\")\n",
        "    new_sentence.append(word)\n",
        "\n",
        "  new_sentences.append(new_sentence)\n",
        "\n",
        "\n",
        "i=0\n",
        "y_test_id = []\n",
        "ages = []\n",
        "for sentence in new_sentences:\n",
        "  # print (\"sentence: \", sentence)\n",
        "\n",
        "  if i != 0:\n",
        "    y_test_id.append(sentence[index_id])\n",
        "    y_test.append(sentence[index_label])\n",
        "    ages.append(sentence[age])\n",
        "  i+= 1\n",
        "\n",
        "\n",
        "print(len(new_sentences))\n",
        "\n",
        "\n",
        "\n",
        "i=0\n",
        "for i in range(len(y_test_id)):\n",
        "  list_obj = [y_test_id[i]]\n",
        "  df_t.loc[df_t['ID'].isin(list_obj), 'prediction'] = y_test[i]\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsFS2Iv4D4Hh",
        "outputId": "63c41ed8-5e87-4f11-8fac-341c16e39721"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Αποθήκευση των dataset σε αρχεία csv\n",
        "\n",
        "--------------------------------------------------\n",
        "\n",
        "Saving the dataset in csv files"
      ],
      "metadata": {
        "id": "eIOUhIFLIy1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"train_good_telos.csv\",index=False)\n",
        "df_t.to_csv(\"test_good_telos.csv\",index=False)"
      ],
      "metadata": {
        "id": "o2KjGYZ1D4gt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}